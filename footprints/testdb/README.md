The *testdb* directory includes the processing code to intersect hint or wellington ouptut with the FIMO database and put the results in a database. This code is in the `src` directory.

The code is an updated version of that used by examples in the `serial_examples` directory, which can be considered legacy code, and aren't actively used any more. The current workflow has been parallelized to enable faster database creation. 

Currently, the workflow is:

- [ ] Make a new folder for running each batch of data by copying all contents from a previous run to the newly created folder (such as the `skin_20` folder). This should include scripts for each footprinting method (e.g. `hint.R`,`wellington.R`), a shell script (e.g. `run_skin_20.sh`), and a `nohup.out` file. 
- [ ] Delete the `nohup.out` file from the new directory
- [ ] Rename and edit the scripts to replace their databases with your new database names and change file paths
- [ ] Add the new databases to [src/dbFunctions.R](https://github.com/PriceLab/BDDS/blob/master/footprints/testdb/src/dbFunctions.R)
- [ ] Add your footprint files
- [ ] Install the necessary R packages
- [ ] Run the database creation shell script using the nohup option to collect output
- [ ] Run the master scripts using the nohup option to collect output
- [ ] Index the databases using the nohup option to collect output
- [ ] Check database contents
- [ ] Make database read only
- [ ] Dump the databases to S3

As a concrete example, here's how to build a database from the hint and wellington output generated by running the [makefile-based tests](https://github.com/PriceLab/BDDS/tree/master/footprints/functionalTests) (e.g. `make hint PYTHON=3` and `make wellington PYTHON=3`).

- [ ] **Make a new folder** by copying from a previous run

From the `BDDS/footprints/testdb` directory:

```
cp -r skin_20 test_dbs
```

Now we have a new folder containing our 4 master scripts (`hint_1/2.R` and `wellington_1/2.R`), our database creation script (`create_dbs_skin_20.sh`), our index creation script (`create_index_skin_20.sh`), our database dumping script (`dump_save_skin_20.sh`), and the output from the previous run (`nohup.out`)

- [ ] **Delete the `nohup.out` file**

The `nohup.out` file is a record of the previous run; each run of a `run_<name>.sh` script generates its own `nohup.out` file, thus you should delete the existing one:

```
rm test_dbs/nohup.out
```

- [ ] **Rename and edit the scripts** 

There are 6 scripts in the folder that carry out several functions: 

- Creating the new databases and their tables (create_dbs_XXXX_XXXX.sh)
- Filling the new databases with footprint data (hint_1.R/hint_2.R/wellington_1.R/wellington_2.R)
- Indexing the databases once they're finished (index_XXXX_XXXX.sql)

In order to create the proper databases and run the proper scripts, it's vital to replace the existing database and file path names with your new ones. For our test case, that means the following substitutions in the `create_dbs_skin_20.sh` file:

line | original | new
---- | -------- | ---
line 6 | was: `create database skin_wellington_20;` | now `create database test_wellington;`
line 7 | was: `grant all privileges on database skin_wellington_20 to trena;` | now `grant all privileges on database test_wellington to trena;`
line 8 | was: `create database skin_hint_20;` | now `create database test_hint;`
line 9 | was: `grant all privileges on database skin_hint_20 to trena;` | now `grant all privileges on database test_hint to trena;`
line 11 | was `\connect skin_wellington_20` | now `\connect test_wellington`

Save the changes in this script; you don't **need** to rename it, but it is highly recommended for clarity. In this case, we'll rename it to `run_test.sh`. Then go ahead and make the same database name changes to the indexing (`create_index_skin_20.sh`) and dumping/saving (`dump_save_skin_20.sh`) scripts.

We'll also need to alter the file paths, database names, and IDs in the master scripts (`hint.R`, `wellington.R`, etc.) 

In your project directory, change the paths, names, and IDs that point to your footprints, databases, and other relevant variables. For example, I will make the following changes in hint_1.R:

line | original | new
---- | -------- | ---
line 12 | was: `data.path <- "/scratch/data/test_set/brain_hint_20"` | now `hint.path <- "/scratch/github/BDDS/footprints/functionalTests/output/hint"`
line 17 | was `db.hint <- "brain_hint_20_localhost"` | now `db.hint <- "test_hint_localhost"`
line 53 | was `minid = "brain_hint_20.minid",` | now `minid = "testexample.filler.minid",`
line 56 | was `dbTable = "brain_hint_20",` | now `dbTable = "test_hint",`

You'll have to make changes in the other 3 master scripts as well. 

Finally, edit the indexing file (e.g. `index_skin_20.sql`) to change the database names:

line | original | new
---- | -------- | ---
line 12 | was: `data.path <- "/scratch/data/test_set/brain_hint_20"` | now `hint.path <- "/scratch/github/BDDS/footprints/functionalTests/output/hint"`
line 17 | was `db.hint <- "brain_hint_20_localhost"` | now `db.hint <- "test_hint_localhost"`

- [ ] **Add the new databases to [src/dbFunctions.R](https://github.com/PriceLab/BDDS/blob/master/footprints/testdb/src/dbFunctions.R)**

Following the existing format in the file, add databases of the same name, using "localhost" as the host if you're running on the same machine as the databases:

```
} else if (database == "test_wellington_localhost") {
    user= "trena"
    password="trena"
    dbname="test_wellington"
    host="localhost"

} else if (database == "test_hint_localhost") {
    user= "trena"
    password="trena"
    dbname="test_hint"
    host="localhost"
}
```

Note that the `"database"` variable is simply a string name, not a parameter used in the database connection. It is crucial that the 4 parameters match your database connection. In the above case, we're using the user `trena` with password `trena` on the `test_wellington` and `test_hint` databases, both of which are on the local machine. 

- [ ] Add your footprint files

In order to perform the intersection of the fimo databasae and the hint/wellington/piq footprints, you'll need the footprint files. A sampling of these files, for seeds 16 and 20 of brain/lymphoblash/skin and the HINT/WELLINGTON methods, can be grabbed from Amazon S3 and copied into your directory (normally /scratch/data/footprints on an EC2 instance):

`aws s3 cp s3://marichards/footprints . --recursive`

In any case, be sure that all your footprints end with `.bed`, all lowercase, else the scripts won't work properly. 

- [ ] Install the necessary R packages

The R scripts for filling the databases require a number of packages; the easiest way to get them is through Bioconductor. Start up R and run the following:

```
source("https://bioconductor.org/biocLite.R")
biocLite(c("BiocParallel","GenomicRanges","RUnit","RPostgreSQL"))
```

Once those 4 packages have successfully installed, you should be ready to run the scripts. 

- [ ] Run the database creation shell script

You'll need to create your databases first or else there will be nothing there to fill. Do so by running your altered database creation script:

```
./create_dbs_test.sh
```

- [ ] Run the hint and wellington master scripts using the nohup option to collect output

In the interest of not overloading your machine, the master scripts are each split into 2 pieces. Our recommendation, particularly for tissues with large numbers of footprinting files (e.g. skin), is to run `hint_1.R` and `wellington_1.R` concurrently, followed by `hint_2.R` and `wellington_2.R`. You can do this from the project directory as follows (for the first 2)

```
nohup R -f hint_1.R &
nohup R -f wellington_1.R &
```

These scripts will now run in the background, where you can monitor them while accomplishing other tasks. Once the first 2 scripts are finished, check the `nohup.out` file created and make sure there are no errors. Near the end of the script, there should be a couple of lines that look something like this:

```
TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
```

Each of these `TRUE`s marks a job that successfully completed, so there should be 10 each for HINT and WELLINGTON. Once you've verified that there were no errors, go ahead and run the second set of master scripts:

```
nohup hint_2.R &
nohup wellington_2.R &
```

Again, these will run in the background. Once you're finished, check again to make sure the jobs completed without error; this time, there should be 14 "TRUE" statements each. 

- [ ] **Index the databases**

It's vital to index your finished database so lookups can be relatively quick and easy. You should have already altered the indexing script so it's tailored to your tissue type. Run it in the background, as it'll take a while for tissues with lots of footprints:

```
nohup psql test_wellington -f index_test_20.sql &
```

- [ ] **Check database contents**

It's a good idea to look at your databases and make sure they're correct. On the Price Lab machine Whovian, here's how I'd look at the 2 test databases:

```
psql test_wellington -U trena -h whovian
select * from hits limit 10;
select * from regions limit 10;

\connect test_hint
select * from hits limit 10;
select * from regions limit 10;
\q
```

- [ ] **Make database read only**
```
psql testwellington -U trena -h whovian
revoke insert, update, delete, truncate on hits from public;
revoke insert, update, delete, truncate on hits from trena;

\connect testhint
revoke insert, update, delete, truncate on hits from public;
revoke insert, update, delete, truncate on hits from trena;

\q
```

- [ ] **Dump the database to S3**

It's vital that we preserve all the work we've put into these databases, as they serve as important resources to other projects, both within and outside the lab. Presuming that your final database isn't a "test" database, you should save it to an S3 bucket to make it reusable.

First, dump the database (here, we'll do test_hint_20):

`pg_dump -Fc -h localhost -U trena test_hint_20 > ./test_hint_20`

Then, copy the database to S3; here, I'm copying it to my "completed_dbs" bucket:

`aws s3 cp ./test_hint_20.dump s3://marichards/completed_dbs/`

Now the database dump file is saved to S3 and can be restored and shared. 
